# ARF-010: Perform integration testing and validation

## User Stories

**As a** QA engineer validating the review feedback refactoring
**I want** comprehensive integration tests that verify real workflows
**So that** I can confirm the refactoring works correctly in production scenarios

**As a** developer preparing to merge the refactoring
**I want** documented test results proving the changes work
**So that** I have evidence the refactoring maintains existing behavior

**As a** user of buildspec commands
**I want** confidence that the refactored code won't break my workflows
**So that** I can upgrade without fear of regressions

## Acceptance Criteria

### Test Fixture Creation
1. Test fixture epic created at `.epics/test-fixtures/simple-epic/` with:
   - Known input epic specification (simple-epic.epic.yaml)
   - Predefined review feedback artifact (epic-file-review-artifact.md)
   - Expected output files (expected-epic.yaml, expected-ticket.md)
   - README documenting the test fixture

### Integration Test Execution
2. Test 1: buildspec create-epic with epic-file-review
   - Run `buildspec create-epic` command on test fixture
   - Invoke epic-file-review with test review artifact
   - Apply review feedback using refactored code
   - Verify epic YAML contains expected changes from review
   - Verify epic-file-review-updates.md created in artifacts/
   - Verify no unexpected errors in log files

3. Test 2: buildspec create-tickets with epic-review
   - Run `buildspec create-tickets` command on test fixture epic
   - Invoke epic-review with test review artifact
   - Apply review feedback using refactored code
   - Verify epic YAML contains expected changes
   - Verify all ticket markdown files contain expected changes
   - Verify epic-review-updates.md created in artifacts/
   - Verify no unexpected errors in log files

4. Test 3: Fallback documentation creation
   - Simulate Claude failure (mock ClaudeRunner to fail)
   - Verify fallback documentation is created
   - Verify fallback doc has status=completed_with_errors
   - Verify fallback doc includes stdout/stderr

5. Test 4: Missing review artifact handling
   - Run create-epic without review artifact
   - Verify clear error message displayed
   - Verify command fails gracefully (not crash)

6. Test 5: Performance verification
   - Measure review application time
   - Verify completes in < 30 seconds for typical epic (10 tickets)
   - Document performance baseline

### Pass Criteria Validation
7. Epic YAML file contains expected changes from review feedback
8. Ticket markdown files contain expected changes from review feedback
9. Documentation artifact exists with status=completed
10. No unexpected errors in log files (only expected warnings allowed)
11. Performance: Review application completes in < 30 seconds
12. Stdout and stderr logged separately (per epic coordination requirements)
13. Console output provides clear user feedback

### Rollback Strategy Implementation
14. If critical bugs found:
    - Document issues in GitHub issues before proceeding
    - Revert to previous implementation
    - Fix issues and re-run full test suite
15. No merge until all integration tests pass

### Documentation
16. Integration test results documented in artifacts/test-results.md
17. Test fixture documented with README.md explaining how to use it
18. Any issues found documented in ISSUES.md with severity and resolution

## Technical Context

This is the final validation ticket that proves the refactoring works correctly. Unlike unit tests (ARF-009) which mock dependencies, integration tests run real commands end-to-end.

**Test Fixture Design:**

The test fixture should be minimal but realistic:

**simple-epic.epic.yaml:**
```yaml
name: Simple Test Epic
description: Minimal epic for testing review feedback
ticket_count: 3
tickets:
  - id: TEST-001
    title: First test ticket
    description: Simple ticket for testing
  - id: TEST-002
    title: Second test ticket
    description: Another test ticket
  - id: TEST-003
    title: Third test ticket
    description: Final test ticket
```

**epic-file-review-artifact.md:**
```yaml
---
status: completed
review_type: epic-file
---

# Review Feedback

## Critical Issues
- [ ] Add missing acceptance criteria to epic description

## High Priority
- [ ] Clarify ticket dependencies

## Medium Priority
- [ ] Improve ticket descriptions
```

**Expected Changes:**

After applying review feedback:
- Epic YAML should have added fields based on review
- Ticket files should have improved descriptions
- Documentation should explain what was changed

**Test Execution Strategy:**

Run tests in order:
1. Unit tests (ARF-009) - establish baseline
2. Create-epic integration test - verify epic-file-review path
3. Create-tickets integration test - verify epic-review path
4. Error handling tests - verify failure scenarios
5. Performance test - verify acceptable speed

**Pass/Fail Criteria:**

Tests PASS if:
- All expected files created
- Content matches expectations (fuzzy match, not exact)
- No errors in logs (warnings OK)
- Performance acceptable (< 30s)
- Console output clear and helpful

Tests FAIL if:
- Missing expected files
- Wrong content in files
- Unexpected errors in logs
- Poor performance (> 30s)
- Confusing console output
- Any crashes or exceptions

**Rollback Strategy:**

If ANY critical bug is found:
1. Document in GitHub issue with reproduction steps
2. Git revert to previous commit
3. Fix bug in separate branch
4. Re-run ALL tests (unit + integration)
5. Only merge when ALL tests pass

Critical bugs include:
- Data loss (files deleted unexpectedly)
- Crashes or exceptions in normal usage
- Wrong files edited
- Security issues (executing arbitrary code)
- Data corruption (malformed YAML, broken markdown)

**Performance Baseline:**

Current implementation (create_epic.py) completes review feedback in ~10-15 seconds for typical epic. Refactored version should be similar (within 2x).

Measure:
- Time from "Applying review feedback..." to completion
- Include Claude API time
- Don't include user input time

Document baseline for future optimization.

## Dependencies

**Depends on:**
- ARF-007: Refactor create_epic.py to use shared utility
- ARF-008: Integrate review feedback into create_tickets.py
- ARF-009: Create unit tests for review_feedback module

**Blocks:**
- None (final validation ticket)

## Collaborative Code Context

**Provides to:**
- Merge request reviewers: Evidence that refactoring works
- Future developers: Test fixtures for regression testing
- Documentation: Integration test examples

**Consumes from:**
- ARF-007: Refactored create_epic.py code
- ARF-008: Integrated create_tickets.py code
- ARF-009: Unit tests (must pass before integration tests)

**Integrates with:**
- buildspec CLI commands (create-epic, create-tickets)
- Claude API (for real review feedback application)
- File system (for test fixtures and outputs)
- Git (for rollback if needed)

## Function Profiles

No new functions - this ticket validates existing functions through integration testing.

## Automated Tests

### Integration Tests

All tests in this ticket ARE integration tests:

- `test_create_epic_with_epic_file_review()` - Full create-epic workflow with review feedback
- `test_epic_yaml_updated_by_review_feedback()` - Verify epic YAML changes match review
- `test_epic_file_review_documentation_created()` - Verify epic-file-review-updates.md exists and correct
- `test_create_tickets_with_epic_review()` - Full create-tickets workflow with review feedback
- `test_epic_and_tickets_updated_by_review_feedback()` - Verify both epic and tickets changed
- `test_epic_review_documentation_created()` - Verify epic-review-updates.md exists and correct
- `test_fallback_documentation_on_claude_failure()` - Verify fallback created when Claude fails
- `test_error_message_when_review_artifact_missing()` - Verify clear error message
- `test_review_feedback_performance()` - Measure and verify performance < 30s
- `test_stdout_stderr_logged_separately()` - Verify separate log files per coordination requirements
- `test_console_output_provides_feedback()` - Verify console messages clear and helpful

### Manual Tests

Some tests may need manual execution:

- Visual inspection of generated documentation
- User experience evaluation of console output
- End-to-end workflow from epic creation through review to feedback application

### Coverage Target

Integration tests should achieve:
- 100% coverage of happy path workflows
- 100% coverage of critical error paths
- 80% coverage of edge cases

**Test Framework**: pytest for automated tests, manual checklist for UX tests

**Test Commands**:
```bash
# Run all integration tests
uv run pytest tests/integration/test_review_feedback_integration.py -v

# Run specific integration test
uv run pytest tests/integration/test_review_feedback_integration.py::test_create_epic_with_epic_file_review -v

# Run with coverage for full stack
uv run pytest tests/integration/ -v --cov=cli --cov-report=term-missing --cov-report=html

# Run performance test
uv run pytest tests/integration/test_review_feedback_integration.py::test_review_feedback_performance -v -s
```

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Test fixture created at .epics/test-fixtures/simple-epic/
- [ ] Test fixture documented with README.md
- [ ] All 5 integration tests executed successfully
- [ ] create-epic with epic-file-review works correctly
- [ ] Epic YAML file contains expected changes
- [ ] epic-file-review-updates.md created with status=completed
- [ ] create-tickets with epic-review works correctly
- [ ] Both epic and ticket files updated correctly
- [ ] epic-review-updates.md created with status=completed
- [ ] Fallback documentation works when Claude fails
- [ ] Error handling works when review artifact missing
- [ ] Performance verified < 30 seconds
- [ ] Stdout and stderr logged separately
- [ ] Console output provides clear feedback
- [ ] Integration test results documented in artifacts/test-results.md
- [ ] No critical bugs found (or all bugs fixed and retested)
- [ ] All tests passing (11 integration tests)
- [ ] Code reviewed
- [ ] Ready to merge

## Non-Goals

- Load testing with hundreds of tickets (test with typical size: 3-10 tickets)
- Security penetration testing (trust Claude API security)
- Stress testing (disk full, network failures, etc.)
- Browser-based UI testing (CLI only)
- Cross-platform testing (test on primary platform only)
- Backwards compatibility testing with old epic formats (current format only)
- Testing every possible edge case (focus on common scenarios)
- Performance optimization (just verify acceptable, not optimal)
- Creating automated regression test suite (manual tests sufficient for now)
- Testing concurrent review feedback (single-threaded per epic non-goals)
