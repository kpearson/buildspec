# ARF-002: Extract _build_feedback_prompt() helper function

## User Stories

**As a** developer applying review feedback
**I want** a function that dynamically builds prompts based on review type
**So that** the same code can generate different prompts for epic-file vs epic reviews

**As a** system maintainer
**I want** prompt generation logic separated from execution logic
**So that** prompt templates are easy to modify and test independently

## Acceptance Criteria

1. Function `_build_feedback_prompt()` exists in cli/utils/review_feedback.py
2. Function signature matches exactly:
   ```python
   def _build_feedback_prompt(review_content: str, targets: ReviewTargets, builder_session_id: str) -> str
   ```
3. Prompt includes all 8 required sections in order:
   - Documentation requirement (file path from targets.artifacts_dir/targets.updates_doc_name)
   - Task description
   - Review content (verbatim from review_content parameter)
   - Workflow steps
   - What to fix (prioritized list)
   - Important rules (varies by targets.review_type)
   - Example edits
   - Final documentation step
4. Dynamic sections vary correctly based on review_type:
   - "epic-file": Rules focus on epic YAML coordination requirements only
   - "epic": Rules cover both epic YAML and ticket markdown files
5. Prompt includes builder_session_id in documentation frontmatter example
6. Prompt includes targets.reviewer_session_id in documentation frontmatter example
7. Prompt specifies editable files/directories from targets configuration
8. Function has comprehensive docstring explaining:
   - Purpose: Build feedback application prompts dynamically
   - Parameters: review_content, targets, builder_session_id
   - Returns: Formatted prompt string ready for Claude
   - Behavior differences based on review_type
9. Type hints present on all parameters and return value
10. Prompt template is well-formatted with proper markdown headings and structure

## Technical Context

This ticket extracts the prompt building logic that currently exists inline in create_epic.py's apply_review_feedback() function. The key innovation is making the prompt dynamic based on ReviewTargets.review_type:

- **epic-file review**: Focuses only on the epic YAML file. Rules emphasize coordination requirements between tickets. Claude is told to edit only the primary_file (epic YAML).

- **epic review**: Covers both epic YAML and all ticket markdown files. Rules include both epic coordination and ticket quality standards. Claude is told to edit primary_file AND all files in additional_files list.

The prompt must instruct Claude to:
1. Read the review feedback carefully
2. Edit the appropriate files (based on review_type)
3. Apply fixes in priority order (critical first, then high, medium, low)
4. Follow important rules specific to the review type
5. Document all changes in the updates template file

The builder_session_id and reviewer_session_id are included in the prompt so Claude knows what to put in the documentation frontmatter (for traceability).

## Dependencies

**Depends on:**
- ARF-001: Create review_feedback.py utility module with ReviewTargets dataclass

**Blocks:**
- ARF-005: Create main apply_review_feedback() function

## Collaborative Code Context

**Provides to:**
- ARF-005: apply_review_feedback() will call this function to build the prompt before invoking Claude

**Consumes from:**
- ARF-001: ReviewTargets dataclass for configuration and review_type

**Integrates with:**
- The generated prompt string is passed to ClaudeRunner for execution
- Prompt references the template file created by ARF-003

## Function Profiles

### `_build_feedback_prompt(review_content: str, targets: ReviewTargets, builder_session_id: str) -> str`
Constructs a formatted prompt string for Claude to apply review feedback. Takes review content from the review artifact, configuration from ReviewTargets, and session ID from the builder. Returns a multi-section prompt with dynamic content based on review_type. Prompt instructs Claude to edit files, apply fixes in priority order, and document changes.

## Automated Tests

### Unit Tests

- `test_build_feedback_prompt_epic_file_review_type()` - Verify prompt for epic-file review includes only epic YAML in editable files
- `test_build_feedback_prompt_epic_review_type()` - Verify prompt for epic review includes both epic YAML and ticket files
- `test_build_feedback_prompt_includes_review_content()` - Verify review_content parameter is included verbatim in prompt
- `test_build_feedback_prompt_includes_builder_session_id()` - Verify builder_session_id appears in frontmatter example
- `test_build_feedback_prompt_includes_reviewer_session_id()` - Verify targets.reviewer_session_id appears in frontmatter example
- `test_build_feedback_prompt_includes_artifacts_path()` - Verify prompt references targets.artifacts_dir/targets.updates_doc_name
- `test_build_feedback_prompt_includes_all_8_sections()` - Verify all required sections present using regex pattern matching
- `test_build_feedback_prompt_section_order()` - Verify sections appear in correct order
- `test_build_feedback_prompt_epic_file_rules()` - Verify "epic-file" review has epic-specific rules only
- `test_build_feedback_prompt_epic_rules()` - Verify "epic" review has both epic and ticket rules
- `test_build_feedback_prompt_special_characters_escaped()` - Verify review_content with special chars (quotes, newlines) doesn't break prompt formatting
- `test_build_feedback_prompt_empty_review_content()` - Verify function handles empty review_content gracefully
- `test_build_feedback_prompt_long_review_content()` - Verify function handles very long review_content (10000+ chars)
- `test_build_feedback_prompt_markdown_formatting()` - Verify prompt has proper markdown headings (##, ###, etc.)

### Integration Tests

- `test_build_feedback_prompt_with_real_targets()` - Create ReviewTargets with real paths and verify prompt references them correctly
- `test_build_feedback_prompt_roundtrip()` - Verify generated prompt can be parsed and contains expected content when checked programmatically

### Coverage Target
100% coverage for this function (critical for correct prompt generation)

**Test Framework**: pytest

**Test Commands**:
```bash
uv run pytest tests/unit/utils/test_review_feedback.py::TestBuildFeedbackPrompt -v
uv run pytest tests/unit/utils/test_review_feedback.py -v --cov=cli.utils.review_feedback --cov-report=term-missing
```

## Definition of Done

- [ ] All acceptance criteria met
- [ ] _build_feedback_prompt() function implemented in cli/utils/review_feedback.py
- [ ] Function signature matches specification exactly
- [ ] All 8 prompt sections implemented
- [ ] Dynamic behavior works for both review types
- [ ] Comprehensive docstring present
- [ ] Type hints on all parameters and return value
- [ ] All tests passing (16 unit + integration tests)
- [ ] Code coverage is 100%
- [ ] Code reviewed
- [ ] No linting errors from ruff

## Non-Goals

- Executing the prompt (that's ARF-005's responsibility)
- Validating review_content format (assumed to be valid YAML from reviewer)
- Handling Claude API errors (that's ARF-005's responsibility)
- Creating the template documentation file (that's ARF-003)
- Supporting review types beyond "epic-file" and "epic"
- Translating or internationalizing the prompt
- Adding prompt versioning or A/B testing
