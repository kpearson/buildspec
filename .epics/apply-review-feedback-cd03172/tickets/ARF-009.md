# ARF-009: Create unit tests for review_feedback module

## User Stories

**As a** developer maintaining the review_feedback module
**I want** comprehensive unit tests covering all functions and edge cases
**So that** I can refactor with confidence and catch regressions early

**As a** CI/CD system
**I want** automated tests that verify review_feedback module correctness
**So that** pull requests are automatically validated before merge

## Acceptance Criteria

1. New test file `tests/unit/utils/test_review_feedback.py` exists
2. All 13 required test cases from epic are implemented:
   - test_review_targets_creation() - Dataclass instantiation
   - test_review_targets_validation() - Field validation
   - test_build_feedback_prompt_epic_file() - Epic-file review prompt
   - test_build_feedback_prompt_epic() - Epic review prompt
   - test_build_feedback_prompt_special_chars() - Special characters in review content
   - test_create_template_doc() - Template generation
   - test_create_template_doc_directory_exists() - Directory creation handling
   - test_create_fallback_doc() - Fallback documentation
   - test_apply_review_feedback_success() - Successful workflow
   - test_apply_review_feedback_missing_artifact() - Error handling
   - test_apply_review_feedback_claude_failure() - Fallback doc creation
   - test_apply_review_feedback_partial_success() - Partial failures
   - test_concurrent_review_feedback() - Thread safety
3. Tests use pytest framework with proper fixtures and mocking
4. Tests cover both "epic-file" and "epic" review types
5. Tests verify prompt template variations based on review_type
6. Tests verify fallback doc creation when Claude fails
7. Tests verify edge cases (special characters, missing directories, partial failures, concurrency)
8. Tests achieve ≥80% code coverage for cli/utils/review_feedback.py
9. All tests pass when run with `uv run pytest tests/unit/utils/test_review_feedback.py -v`
10. Tests use pytest-mock for mocking external dependencies (ClaudeRunner, file I/O)
11. Tests use temp directories (tmpdir fixture) for file operations
12. Tests are organized into logical test classes (TestReviewTargets, TestBuildFeedbackPrompt, etc.)
13. Each test has clear docstring explaining what it verifies
14. Tests follow AAA pattern (Arrange, Act, Assert)
15. Test data is realistic and represents actual usage patterns

## Technical Context

This ticket creates the comprehensive test suite for the review_feedback module created in ARF-001 through ARF-005. The tests must cover:

**ReviewTargets dataclass (ARF-001):**
- Field instantiation and access
- Type hints presence
- Dataclass behavior (equality, repr, etc.)

**_build_feedback_prompt() (ARF-002):**
- Prompt content for both review types
- Dynamic sections based on review_type
- Special character handling
- All required sections present

**_create_template_doc() (ARF-003):**
- File creation at correct path
- Frontmatter schema correctness
- Directory creation
- UTF-8 encoding

**_create_fallback_updates_doc() (ARF-004):**
- Fallback doc creation
- Status field logic (completed vs completed_with_errors)
- File path detection from stdout
- stdout/stderr inclusion

**apply_review_feedback() (ARF-005):**
- End-to-end workflow orchestration
- Error handling for all scenarios
- ClaudeRunner integration
- Template validation logic
- Fallback creation when needed

**Testing Strategy:**

**Mocking approach:**
- Mock ClaudeRunner to simulate Claude execution
- Mock file I/O where appropriate (but use real files for integration tests)
- Mock yaml parsing failures for error scenarios
- Use pytest-mock's mocker fixture for clean mocking

**Fixture usage:**
- tmpdir: For temporary file operations
- mocker: For mocking external dependencies
- Custom fixtures for common test data (sample ReviewTargets, sample review content)

**Edge cases to test:**
- Empty review content
- Very long review content (>100KB)
- Special characters in paths (spaces, unicode)
- Missing directories
- Permission errors
- Disk full errors
- Concurrent access (multiple threads)
- Malformed YAML
- Invalid review_type values

**Coverage requirements:**
Per test-standards.md, we need:
- Minimum 80% line coverage for new code
- 100% coverage for critical paths (main workflow, error handling)
- All public functions tested
- All error paths tested

## Dependencies

**Depends on:**
- ARF-005: Create main apply_review_feedback() function

**Blocks:**
- ARF-010: Perform integration testing and validation

## Collaborative Code Context

**Provides to:**
- ARF-010: These unit tests provide baseline confidence before integration testing
- Future maintainers: Tests document expected behavior and serve as regression suite

**Consumes from:**
- ARF-001 through ARF-005: All functions being tested

**Integrates with:**
- pytest framework
- pytest-mock plugin
- pytest-cov plugin for coverage reporting

## Function Profiles

No new functions - this ticket creates tests for existing functions.

## Automated Tests

This entire ticket IS about creating automated tests. The tests themselves are the deliverable.

**Test Classes:**

### `TestReviewTargets`
Tests for ReviewTargets dataclass (ARF-001)

- `test_review_targets_creation_with_all_fields()` - Create instance with all fields, verify accessible
- `test_review_targets_type_hints_present()` - Use inspect module to verify type annotations
- `test_review_targets_epic_file_review_type()` - Verify review_type="epic-file" works
- `test_review_targets_epic_review_type()` - Verify review_type="epic" works
- `test_review_targets_path_fields_accept_path_objects()` - Verify Path fields work correctly
- `test_review_targets_additional_files_empty_list()` - Verify empty additional_files
- `test_review_targets_editable_directories_empty_list()` - Verify empty editable_directories
- `test_review_targets_string_representation()` - Verify __repr__ output
- `test_review_targets_equality()` - Verify dataclass equality comparison
- `test_review_targets_with_real_paths()` - Integration test with real temp paths

### `TestBuildFeedbackPrompt`
Tests for _build_feedback_prompt() (ARF-002)

- `test_build_feedback_prompt_epic_file_review_type()` - Verify epic-file prompt structure
- `test_build_feedback_prompt_epic_review_type()` - Verify epic prompt structure
- `test_build_feedback_prompt_includes_review_content()` - Verify review_content included
- `test_build_feedback_prompt_includes_builder_session_id()` - Verify builder session ID in prompt
- `test_build_feedback_prompt_includes_reviewer_session_id()` - Verify reviewer session ID in prompt
- `test_build_feedback_prompt_includes_artifacts_path()` - Verify artifact path references
- `test_build_feedback_prompt_includes_all_8_sections()` - Verify all required sections
- `test_build_feedback_prompt_section_order()` - Verify sections in correct order
- `test_build_feedback_prompt_epic_file_rules()` - Verify epic-file specific rules
- `test_build_feedback_prompt_epic_rules()` - Verify epic specific rules (both epic + tickets)
- `test_build_feedback_prompt_special_characters_escaped()` - Test with quotes, newlines, etc.
- `test_build_feedback_prompt_empty_review_content()` - Verify handling of empty content
- `test_build_feedback_prompt_long_review_content()` - Test with 10000+ char content
- `test_build_feedback_prompt_markdown_formatting()` - Verify markdown structure

### `TestCreateTemplateDoc`
Tests for _create_template_doc() (ARF-003)

- `test_create_template_doc_creates_file()` - Verify file created at correct path
- `test_create_template_doc_includes_frontmatter()` - Verify frontmatter present and valid
- `test_create_template_doc_frontmatter_date_format()` - Verify YYYY-MM-DD date
- `test_create_template_doc_frontmatter_epic_name()` - Verify epic field
- `test_create_template_doc_frontmatter_builder_session_id()` - Verify builder session ID
- `test_create_template_doc_frontmatter_reviewer_session_id()` - Verify reviewer session ID
- `test_create_template_doc_frontmatter_status_in_progress()` - Verify status="in_progress"
- `test_create_template_doc_includes_placeholder_sections()` - Verify placeholder sections
- `test_create_template_doc_creates_parent_directories()` - Verify directory creation
- `test_create_template_doc_overwrites_existing_file()` - Verify overwrite behavior
- `test_create_template_doc_utf8_encoding()` - Test with unicode characters
- `test_create_template_doc_roundtrip()` - Create and read back, verify parseable

### `TestCreateFallbackDoc`
Tests for _create_fallback_updates_doc() (ARF-004)

- `test_create_fallback_doc_creates_file()` - Verify file created
- `test_create_fallback_doc_frontmatter_status_with_errors()` - Status when stderr present
- `test_create_fallback_doc_frontmatter_status_completed()` - Status when stderr empty
- `test_create_fallback_doc_includes_stdout()` - Verify stdout in doc
- `test_create_fallback_doc_includes_stderr()` - Verify stderr in doc when present
- `test_create_fallback_doc_omits_stderr_section_when_empty()` - No stderr section when empty
- `test_create_fallback_doc_detects_edited_files()` - Parse "Edited file:" pattern
- `test_create_fallback_doc_detects_written_files()` - Parse "Wrote file:" pattern
- `test_create_fallback_doc_deduplicates_file_paths()` - Same file listed once
- `test_create_fallback_doc_empty_stdout()` - Handle empty stdout
- `test_create_fallback_doc_includes_next_steps()` - Verify next steps guidance
- `test_create_fallback_doc_utf8_encoding()` - Test with unicode
- `test_create_fallback_doc_roundtrip()` - Create and read back

### `TestApplyReviewFeedback`
Tests for apply_review_feedback() (ARF-005)

- `test_apply_review_feedback_success_epic_file()` - Full workflow for epic-file review
- `test_apply_review_feedback_success_epic()` - Full workflow for epic review
- `test_apply_review_feedback_missing_review_artifact()` - FileNotFoundError handling
- `test_apply_review_feedback_malformed_yaml()` - yaml.YAMLError handling
- `test_apply_review_feedback_claude_failure_creates_fallback()` - ClaudeRunner error handling
- `test_apply_review_feedback_template_not_updated_creates_fallback()` - Fallback when status=in_progress
- `test_apply_review_feedback_logs_stdout_stderr()` - Verify logging
- `test_apply_review_feedback_logs_errors()` - Verify error logging
- `test_apply_review_feedback_console_output_success()` - Verify success messages
- `test_apply_review_feedback_console_output_failure()` - Verify failure messages
- `test_apply_review_feedback_calls_helper_functions()` - Verify orchestration
- `test_apply_review_feedback_builds_prompt_with_correct_params()` - Verify parameter passing
- `test_apply_review_feedback_creates_template_before_claude()` - Verify order
- `test_apply_review_feedback_validates_frontmatter_status()` - Verify validation logic
- `test_apply_review_feedback_end_to_end_with_real_files()` - Integration test

### Coverage Target
≥80% minimum for review_feedback.py, targeting 100% for critical paths

**Test Commands**:
```bash
# Run all tests
uv run pytest tests/unit/utils/test_review_feedback.py -v

# Run specific test class
uv run pytest tests/unit/utils/test_review_feedback.py::TestApplyReviewFeedback -v

# Run with coverage
uv run pytest tests/unit/utils/test_review_feedback.py -v --cov=cli.utils.review_feedback --cov-report=term-missing --cov-report=html

# Run fast (skip slow tests if marked)
uv run pytest tests/unit/utils/test_review_feedback.py -v -m "not slow"
```

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Test file tests/unit/utils/test_review_feedback.py created
- [ ] All 13 required test cases implemented (plus additional tests from detailed list)
- [ ] Tests organized into logical test classes
- [ ] Tests use pytest with proper fixtures and mocking
- [ ] Tests cover both epic-file and epic review types
- [ ] Tests verify prompt variations
- [ ] Tests verify fallback doc creation
- [ ] Tests cover edge cases and error scenarios
- [ ] Code coverage ≥80% for review_feedback.py
- [ ] All tests pass
- [ ] Tests follow AAA pattern
- [ ] Each test has clear docstring
- [ ] Test data is realistic
- [ ] Code reviewed
- [ ] No linting errors from ruff

## Non-Goals

- Integration tests (that's ARF-010)
- Performance benchmarking (unit tests should be fast, but no specific benchmarks)
- Testing ClaudeRunner implementation (mock it instead)
- Testing third-party libraries (yaml, pathlib, etc.)
- Testing Python language features
- Creating test fixtures for integration tests (ARF-010's responsibility)
- Testing CLI argument parsing (not part of review_feedback module)
- Testing console output formatting details (verify messages are sent, not exact formatting)
- Creating mutation tests or property-based tests (nice to have but not required)
